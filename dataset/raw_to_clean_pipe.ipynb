{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34104b1",
   "metadata": {},
   "source": [
    "## Pipeline from raw data to df ready for train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bab257c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea003e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory: /Users/angie/code/finalproject/project_accidents/data\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 1. PROJECT ROOT\n",
    "# ------------------------------------------------------\n",
    "\n",
    "cwd = os.getcwd()\n",
    "if os.path.basename(cwd) == \"dataset\":\n",
    "    PROJECT_ROOT = os.path.dirname(cwd)\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23bbb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 2. Helper function to load all CSVs matching a prefix\n",
    "# ------------------------------------------------------\n",
    "def load_all_years(prefix):\n",
    "    \"\"\"\n",
    "    Loads all CSVs where filename starts with prefix + '-' (e.g., caracteristiques-2020.csv).\n",
    "    Returns a single concatenated DataFrame.\n",
    "    \"\"\"\n",
    "    files = [\n",
    "        f for f in os.listdir(RAW_DATA_DIR)\n",
    "        if f.startswith(prefix + \"-\") and f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found for prefix '{prefix}' in /data\")\n",
    "\n",
    "    dfs = []\n",
    "    for file in sorted(files):   # sorted ensures year order\n",
    "        path = os.path.join(RAW_DATA_DIR, file)\n",
    "        print(f\"  → Loading {file}\")\n",
    "        dfs.append(pd.read_csv(path, sep=\";\"))\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"{prefix}: Loaded {combined.shape[0]:,} rows\")\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4ce2301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Loading caracteristiques-2019.csv\n",
      "  → Loading caracteristiques-2020.csv\n",
      "  → Loading caracteristiques-2021.csv\n",
      "  → Loading caracteristiques-2022.csv\n",
      "  → Loading caracteristiques-2023.csv\n",
      "caracteristiques: Loaded 273,226 rows\n",
      "  → Loading lieux-2019.csv\n",
      "  → Loading lieux-2020.csv\n",
      "  → Loading lieux-2021.csv\n",
      "  → Loading lieux-2022.csv\n",
      "  → Loading lieux-2023.csv\n",
      "lieux: Loaded 289,264 rows\n",
      "  → Loading usagers-2019.csv\n",
      "  → Loading usagers-2020.csv\n",
      "  → Loading usagers-2021.csv\n",
      "  → Loading usagers-2022.csv\n",
      "  → Loading usagers-2023.csv\n",
      "usagers: Loaded 619,971 rows\n",
      "\n",
      "All raw files loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 3. LOAD RAW DATA\n",
    "# ------------------------------------------------------\n",
    "details = load_all_years(\"caracteristiques\")\n",
    "places  = load_all_years(\"lieux\")\n",
    "users   = load_all_years(\"usagers\")\n",
    "\n",
    "print(\"\\nAll raw files loaded successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4708e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4. ADD MAX GRAV AND PEOPLE INVOLVED TO USERS DF\n",
    "# ------------------------------------------------------\n",
    "\n",
    "#Count number of users involved per accident\n",
    "users[\"users_involved\"] = (\n",
    "    users.groupby(\"Num_Acc\")[\"Num_Acc\"].transform(\"count\")\n",
    ")\n",
    "\n",
    "# Select row with max 'grav' per accident\n",
    "users = users.loc[\n",
    "    users.groupby(\"Num_Acc\")[\"grav\"].idxmax()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e44b2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: 289,264 rows × 49 columns\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 5. MERGE ALL DATASETS\n",
    "# ------------------------------------------------------\n",
    "\n",
    "df = (\n",
    "    places\n",
    "    .merge(users, on=\"Num_Acc\", how=\"left\")\n",
    "    .merge(details, on=\"Num_Acc\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f87320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed columns\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 6. RENAME COLUMNS\n",
    "# ------------------------------------------------------\n",
    "\n",
    "df.rename(columns={\n",
    "    'Num_Acc': 'accident_number',\n",
    "    'jour': 'day',\n",
    "    'mois': 'month',\n",
    "    'an': 'year',\n",
    "    'hrmn': 'hour_minute',\n",
    "    'lum': 'light_conditions',\n",
    "    'dep': 'department',\n",
    "    'com': 'commune',\n",
    "    'agg': 'urban_area',\n",
    "    'int': 'intersection_type',\n",
    "    'atm': 'weather',\n",
    "    'col': 'collision_type',\n",
    "    'adr': 'road_address',\n",
    "    'lat': 'latitude',\n",
    "    'long': 'longitude',\n",
    "    'Accident_Id': 'accident_uid',\n",
    "    'catr': 'road_category',\n",
    "    'voie': 'road_number',\n",
    "    'v1': 'numerical_index_road',\n",
    "    'v2': 'alphanumeric_index_road',\n",
    "    'circ': 'road_layout',\n",
    "    'nbv': 'num_lanes',\n",
    "    'vosp': 'reserved_lane',\n",
    "    'prof': 'road_profile',\n",
    "    'pr': 'road_ref_1',\n",
    "    'pr1': 'road_ref_2',\n",
    "    'plan': 'road_shape',\n",
    "    'lartpc': 'width_central_reservation',\n",
    "    'larrout': 'width_carriageway',\n",
    "    'surf': 'surface_condition',\n",
    "    'infra': 'infrastructure',\n",
    "    'situ': 'road_location',\n",
    "    'vma': 'speed_limit',\n",
    "    'id_vehicule': 'vehicle_id',\n",
    "    'num_veh': 'vehicle_number',\n",
    "    'place': 'seat_position',\n",
    "    'catu': 'user_category',\n",
    "    'grav': 'injury_severity',\n",
    "    'sexe': 'sex',\n",
    "    'an_nais': 'birth_year',\n",
    "    'trajet': 'trip_purpose',\n",
    "    'secu1': 'safety_device_1',\n",
    "    'secu2': 'safety_device_2',\n",
    "    'secu3': 'safety_device_3',\n",
    "    'locp': 'pedestrian_location',\n",
    "    'actp': 'pedestrian_action',\n",
    "    'etatp': 'pedestrian_alone',\n",
    "    'id_usager': 'user_id'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"Renamed columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dfe93d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned date columns\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 6. CLEAN DATE COLUMNS\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Combine day, month, year columns\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "# Drop rows where 'date' is missing. These rows are all missing lat/long data as well.\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Create day of the week column\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "\n",
    "# Drop old columns\n",
    "df.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Extract hour and rename column\n",
    "df['hour'] = df['hour_minute'].str.split(':').str[0].astype(int)\n",
    "\n",
    "# Drop the original 'hour_minute' column\n",
    "df = df.drop(columns=['hour_minute'])\n",
    "\n",
    "# Move new columns to the front\n",
    "cols = ['date', 'day_of_week', 'hour'] + [c for c in df.columns if c not in ['date', 'day_of_week', 'hour']]\n",
    "df = df[cols]\n",
    "\n",
    "print(f\"Cleaned date columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "20c4af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped unnecessary columns from users, places, and details datasets.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 7. DROP IRRELEVANT COLUMNS FROM THE TABLE\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Columns removed from each dataset\n",
    "cols_to_drop = {\n",
    "    \"users dataset\": [\n",
    "        'vehicle_id', 'vehicle_number', 'seat_position', 'user_category', 'sex',\n",
    "        'birth_year', 'trip_purpose', 'safety_device_1', 'safety_device_2',\n",
    "        'safety_device_3', 'pedestrian_location', 'pedestrian_action',\n",
    "        'pedestrian_alone', 'user_id'\n",
    "    ],\n",
    "    \"places dataset\": [\n",
    "        'road_number', 'numerical_index_road', 'alphanumeric_index_road',\n",
    "        'road_ref_1', 'road_ref_2', 'width_central_reservation',\n",
    "        'width_carriageway'\n",
    "    ],\n",
    "    \"details dataset\": [\n",
    "        'accident_uid', 'road_address', 'commune'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten all columns into one list\n",
    "all_drop_cols = [col for cols in cols_to_drop.values() for col in cols]\n",
    "\n",
    "# Drop them in one go (ignoring missing columns just in case)\n",
    "df = df.drop(columns=all_drop_cols, errors='ignore')\n",
    "\n",
    "print(\"Dropped unnecessary columns from users, places, and details datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0d545dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recategorized columns\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# 8. RECATEGORIZING COLUMNS\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Recategorize road_category column\n",
    "\n",
    "road_category_mapping = {\n",
    "    1: 'Major Roads',\n",
    "    2: 'Major Roads',\n",
    "    3: 'Secondary Roads',\n",
    "    7: 'Secondary Roads',\n",
    "    4: 'Local & Access Roads',\n",
    "    6: 'Local & Access Roads',\n",
    "    5: 'Other / Off-Network',\n",
    "    9: 'Other / Off-Network'\n",
    "}\n",
    "\n",
    "df['road_category'] = df['road_category'].map(road_category_mapping)\n",
    "\n",
    "\n",
    "# Recategorize road layout column\n",
    "# Unknown values impluted with 'Two Way' (most common value)\n",
    "\n",
    "road_layout_mapping = {\n",
    "    -1: 'Two Way',\n",
    "    1: 'One Way',\n",
    "    2: 'Two Way',\n",
    "    3: 'Multi Lane',\n",
    "    4: 'Multi Lane'\n",
    "}\n",
    "\n",
    "df['road_layout'] = df['road_layout'].map(road_layout_mapping)\n",
    "\n",
    "\n",
    "# Convert num_lanes from object to int\n",
    "def clean_to_int(x):\n",
    "    try:\n",
    "        # Remove whitespace, then convert to int\n",
    "        return int(str(x).strip())\n",
    "    except:\n",
    "        # If conversion fails, classify as -1\n",
    "        return -1\n",
    "\n",
    "df['num_lanes'] = df['num_lanes'].apply(clean_to_int)\n",
    "\n",
    "# Replace lanes that are 0 or -1 with 2 (most common value)\n",
    "df.loc[(df['num_lanes'] < 1), 'num_lanes'] = 2\n",
    "\n",
    "\n",
    "# Recategorize reserved_lane column\n",
    "# Unknown values impluted with 'No value' (most common value)\n",
    "\n",
    "reserved_lane_mapping = {\n",
    "    -1: 'No value',\n",
    "    0: 'No value',\n",
    "    1: 'Cycle Lane',\n",
    "    2: 'Cycle Lane',\n",
    "    3: 'Reserved Lane'\n",
    "}\n",
    "\n",
    "df['reserved_lane'] = df['reserved_lane'].map(reserved_lane_mapping)\n",
    "\n",
    "\n",
    "# Recategorize road_profile column\n",
    "# Unknown values impluted with 'flat' (most common value)\n",
    "\n",
    "road_profile_mapping = {\n",
    "    -1: 'Flat',\n",
    "    1: 'Flat',\n",
    "    2: 'Slope / Near Slope',\n",
    "    3: 'Slope / Near Slope',\n",
    "    4: 'Slope / Near Slope'\n",
    "}\n",
    "\n",
    "df['road_profile'] = df['road_profile'].map(road_profile_mapping)\n",
    "\n",
    "\n",
    "# Recategorize road_shape column\n",
    "# Unknown values impluted with 'straight' (most common value)\n",
    "\n",
    "road_shape_mapping = {\n",
    "    -1: 'Straight',\n",
    "    1: 'Straight',\n",
    "    2: 'Curved',\n",
    "    3: 'Curved',\n",
    "    4: 'Curved'\n",
    "}\n",
    "\n",
    "df['road_shape'] = df['road_shape'].map(road_shape_mapping)\n",
    "\n",
    "\n",
    "# Recategorize surface_condition column\n",
    "# Unknown values impluted with 'normal' (most common value)\n",
    "\n",
    "surface_condition_mapping = {\n",
    "    -1: 'Normal',\n",
    "    1: 'Normal',\n",
    "    2: 'Wet / Slippery',\n",
    "    3: 'Wet / Slippery',\n",
    "    4: 'Wet / Slippery',\n",
    "    5: 'Wet / Slippery',\n",
    "    6: 'Wet / Slippery',\n",
    "    7: 'Wet / Slippery',\n",
    "    8: 'Wet / Slippery',\n",
    "    9: 'Wet / Slippery'\n",
    "}\n",
    "\n",
    "df['surface_condition'] = df['surface_condition'].map(surface_condition_mapping)\n",
    "\n",
    "\n",
    "# Recategorize infrastructure column\n",
    "# Unknown values impluted with 'No value' (most common value)\n",
    "\n",
    "infrastructure_mapping = {\n",
    "    -1: 'No value',\n",
    "    0: 'No value',\n",
    "    1: 'Tunnel / Bridge',\n",
    "    2: 'Tunnel / Bridge',\n",
    "    3: 'Intersections',\n",
    "    4: 'Intersections',\n",
    "    5: 'Intersections',\n",
    "    6: 'Intersections',\n",
    "    7: 'Other',\n",
    "    8: 'Other',\n",
    "    9: 'Other'\n",
    "}\n",
    "\n",
    "df['infrastructure'] = df['infrastructure'].map(infrastructure_mapping)\n",
    "\n",
    "\n",
    "# Recategorize road_location column\n",
    "# Unknown values impluted with 'Road' (most common value)\n",
    "\n",
    "road_location_mapping = {\n",
    "    -1: 'Road',\n",
    "    0: 'Road',\n",
    "    1: 'Road',\n",
    "    2: 'Reserved Lanes',\n",
    "    3: 'Reserved Lanes',\n",
    "    4: 'Cyclist / Pedestrian',\n",
    "    5: 'Cyclist / Pedestrian',\n",
    "    6: 'Reserved Lanes',\n",
    "    8: 'Other'\n",
    "}\n",
    "\n",
    "df['road_location'] = df['road_location'].map(road_location_mapping)\n",
    "\n",
    "\n",
    "# Cleaning the speed limit column\n",
    "# Round 'speed_limit' to nearest 10\n",
    "df['speed_limit'] = ((df['speed_limit'] / 10).round(0) * 10).astype(int)\n",
    "\n",
    "# There are rows where speed limit is between 130 and 200. Impute it with 130, assuming these are highways.\n",
    "df.loc[(df['speed_limit'] > 130) & (df['speed_limit'] < 200), 'speed_limit'] = 130\n",
    "\n",
    "# There are rows where speed limit is over 200. Impute it with the median speed (50kmh), assuming these are input errors.\n",
    "median_speed = df[df['speed_limit']<=130]['speed_limit'].median()\n",
    "\n",
    "df.loc[(df['speed_limit'] > 130), 'speed_limit'] = median_speed\n",
    "\n",
    "# Impute missing speed limits with 50.\n",
    "df.loc[(df['speed_limit'] < 1), 'speed_limit'] = 50\n",
    "\n",
    "\n",
    "# Recategorize light_conditions column\n",
    "\n",
    "light_conditions_mapping = {\n",
    "    1: 'Day',\n",
    "    2: 'Twilight',\n",
    "    3: 'Night',\n",
    "    4: 'Night',\n",
    "    5: 'Night'\n",
    "}\n",
    "\n",
    "df['light_conditions'] = df['light_conditions'].map(light_conditions_mapping)\n",
    "\n",
    "# 6 rows with missing light conditions\n",
    "def classify_light_condition(hour):\n",
    "    if 7 <= hour <= 18:\n",
    "        return \"Day\"\n",
    "    elif 5 <= hour <= 6 or 19 <= hour <= 20:\n",
    "        return \"Twilight\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "mask = df['light_conditions'].isna()\n",
    "\n",
    "df.loc[mask, 'light_conditions'] = (\n",
    "    df.loc[mask, 'hour']\n",
    "            .apply(classify_light_condition)\n",
    ")\n",
    "\n",
    "\n",
    "# Keep only rows with numeric department codes\n",
    "# This removes 2251 rows corresponding to Corsica ('2A'/'2B') and overseas territories\n",
    "df = df[df['department'].astype(str).str.isdigit().fillna(False)]\n",
    "\n",
    "# Define Île-de-France department codes\n",
    "idf_departments = {75, 77, 78, 91, 92, 93, 94, 95}\n",
    "\n",
    "# Convert to integer\n",
    "df['department'] = df['department'].astype(int)\n",
    "\n",
    "# Keep only IDF rows\n",
    "df = df[df['department'].isin(idf_departments)]\n",
    "\n",
    "\n",
    "# Rename value of urban_area column\n",
    "urban_area_mapping = {\n",
    "    1: 'Outside urban area',\n",
    "    2: 'Inside urban area'\n",
    "}\n",
    "\n",
    "df['urban_area'] = df['urban_area'].map(urban_area_mapping)\n",
    "\n",
    "\n",
    "# Recategorize intersection_type column\n",
    "# Unknown values impluted with 'No junction' (most common value)\n",
    "\n",
    "intersection_type_mapping = {\n",
    "    -1: 'No junction',\n",
    "    1: 'No junction',\n",
    "    2: 'Simple junction',\n",
    "    3: 'Simple junction',\n",
    "    4: 'Simple junction',\n",
    "    5: 'Complex junction',\n",
    "    6: 'Complex junction',\n",
    "    7: 'Complex junction',\n",
    "    8: 'Other junction',\n",
    "    9: 'Other junction'\n",
    "}\n",
    "\n",
    "df['intersection_type'] = df['intersection_type'].map(intersection_type_mapping)\n",
    "\n",
    "\n",
    "# Recategorize weather column based on visibiltiy and road condition\n",
    "# Unknown values impluted with 'Normal' (most common value)\n",
    "\n",
    "weather_mapping = {\n",
    "    -1: 'Normal Visibility',\n",
    "     1: 'Normal Visibility',\n",
    "     8: 'Normal Visibility',\n",
    "     2: 'Reduced Traction',\n",
    "     3: 'Reduced Traction',\n",
    "     4: 'Reduced Traction',\n",
    "     5: 'Reduced Visibility',\n",
    "     6: 'Normal Visibility',\n",
    "     7: 'Normal Visibility',\n",
    "     9: 'Reduced Visibility'\n",
    "}\n",
    "\n",
    "df['weather'] = df['weather'].map(weather_mapping)\n",
    "\n",
    "\n",
    "# Recategorize collision_type column\n",
    "# Unknown values impluted with '2-car collision' (most common value)\n",
    "\n",
    "collision_type_mapping = {\n",
    "    -1: '2-car collision',  # Not specified\n",
    "     1: '2-car collision',  # Head-on\n",
    "     2: '2-car collision',  # Rear-end\n",
    "     3: '2-car collision',  # Side collision\n",
    "     4: 'Multi-car collision', # Chain collision\n",
    "     5: 'Multi-car collision', # Multiple collisions\n",
    "     6: 'Multi-car collision', # Other collision\n",
    "     7: 'No collision'      # No collision\n",
    "}\n",
    "\n",
    "df['collision_type'] = df['collision_type'].map(collision_type_mapping)\n",
    "\n",
    "\n",
    "# Convert latitude and longitude from strings with comma decimal separator to float\n",
    "df['latitude'] = df['latitude'].str.replace(',', '.').astype(float)\n",
    "df['longitude'] = df['longitude'].str.replace(',', '.').astype(float)\n",
    "\n",
    "print('Recategorized columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 9. DROP DUPLICATE ROWS\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Remove fully duplicated rows to ensure each row is unique across all columns\n",
    "# This will avoid data leakage\n",
    "df = df.drop_duplicates(keep='first')\n",
    "\n",
    "print(f'Dropped duplicate rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "441a4d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70062, 24)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_accidents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
